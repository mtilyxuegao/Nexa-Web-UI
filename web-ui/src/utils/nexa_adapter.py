"""
Nexa AI LLM Adapter for Web-UI
ä¸“é—¨å¤„ç† Nexa SDK çš„ LLM é›†æˆï¼Œæ”¯æŒå¤šæ¨¡æ€å†…å®¹è½¬æ¢
"""

import requests
import json
import base64
import tempfile
import os
import asyncio
from pathlib import Path
from typing import List, Optional, Any, Dict, Union, AsyncIterator, Iterator
from langchain_openai import ChatOpenAI
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage
from langchain_core.outputs import ChatResult, ChatGeneration, LLMResult
from langchain_core.callbacks.manager import CallbackManagerForLLMRun, AsyncCallbackManagerForLLMRun
from langchain_core.language_models.base import LanguageModelInput
from langchain_core.runnables import RunnableConfig
from pydantic import Field
import logging

logger = logging.getLogger(__name__)


class NexaChatLLM(ChatOpenAI):
    """
    Nexa AI èŠå¤©æ¨¡å‹é€‚é…å™¨
    ç»§æ‰¿è‡ª ChatOpenAIï¼Œé‡å†™æ‰€æœ‰è°ƒç”¨æ–¹æ³•æ¥æ‹¦æˆªå¤šæ¨¡æ€è¯·æ±‚å¹¶è½¬æ¢ base64 å›¾ç‰‡
    """
    
    nexa_base_url: str = Field(description="Nexa API åŸºç¡€URL")
    nexa_model: str = Field(description="Nexa æ¨¡å‹åç§°")
    temp_dir: str = Field(default_factory=lambda: tempfile.mkdtemp(prefix="nexa_images_"))
    
    def __init__(self, model: str, base_url: str, temperature: float = 0.0, **kwargs):
        # åˆå§‹åŒ–çˆ¶ç±»ï¼Œä½†ä½¿ç”¨è™šå‡çš„ API key å› ä¸º Nexa ä¸éœ€è¦
        super().__init__(
            model=model,
            base_url=base_url,
            api_key="not-required",  # Nexa ä¸éœ€è¦ API key
            temperature=temperature,
            nexa_base_url=base_url.rstrip('/'),
            nexa_model=model,
            **kwargs
        )
        
        # åˆ›å»ºä¸´æ—¶ç›®å½•ç”¨äºå­˜å‚¨è½¬æ¢çš„å›¾ç‰‡
        os.makedirs(self.temp_dir, exist_ok=True)
        logger.info(f"ğŸ–¼ï¸ Nexa adapter åˆå§‹åŒ–å®Œæˆï¼Œå›¾ç‰‡ä¸´æ—¶ç›®å½•ï¼š{self.temp_dir}")
    
    def __del__(self):
        """æ¸…ç†ä¸´æ—¶ç›®å½•"""
        try:
            import shutil
            if hasattr(self, 'temp_dir') and os.path.exists(self.temp_dir):
                shutil.rmtree(self.temp_dir)
                logger.info(f"ğŸ—‘ï¸ æ¸…ç†ä¸´æ—¶ç›®å½•ï¼š{self.temp_dir}")
        except:
            pass
    
    def _convert_base64_to_file(self, base64_data: str, file_extension: str = "png") -> str:
        """å°† base64 å›¾ç‰‡æ•°æ®è½¬æ¢ä¸ºæœ¬åœ°æ–‡ä»¶è·¯å¾„"""
        try:
            logger.info(f"ğŸ”„ å¼€å§‹è½¬æ¢ base64 å›¾ç‰‡æ•°æ®...")
            
            # ç§»é™¤ data URL å‰ç¼€ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if base64_data.startswith('data:'):
                # æå–æ–‡ä»¶ç±»å‹
                if 'image/jpeg' in base64_data or 'image/jpg' in base64_data:
                    file_extension = "jpg"
                elif 'image/png' in base64_data:
                    file_extension = "png"
                elif 'image/webp' in base64_data:
                    file_extension = "webp"
                
                base64_data = base64_data.split(',', 1)[1]
            
            # è§£ç  base64 æ•°æ®
            image_data = base64.b64decode(base64_data)
            
            # åœ¨ä¸´æ—¶ç›®å½•ä¸­åˆ›å»ºæ–‡ä»¶
            import time
            filename = f"image_{int(time.time() * 1000)}.{file_extension}"
            file_path = os.path.join(self.temp_dir, filename)
            
            # å†™å…¥æ–‡ä»¶
            with open(file_path, 'wb') as f:
                f.write(image_data)
            
            logger.info(f"âœ… base64 å›¾ç‰‡å·²è½¬æ¢ä¸ºï¼š{file_path} ({len(image_data)} bytes)")
            return file_path
            
        except Exception as e:
            logger.error(f"âŒ base64 è½¬æ¢å¤±è´¥: {e}")
            raise ValueError(f"æ— æ³•è½¬æ¢ base64 å›¾ç‰‡æ•°æ®: {e}")
    
    def _process_multimodal_content(self, content: Any) -> Any:
        """å¤„ç†å¤šæ¨¡æ€å†…å®¹ï¼Œå°† base64 å›¾ç‰‡è½¬æ¢ä¸ºæ–‡ä»¶è·¯å¾„"""
        if isinstance(content, list):
            # å¤„ç†å¤šæ¨¡æ€å†…å®¹åˆ—è¡¨
            processed_content = []
            for item in content:
                if isinstance(item, dict) and item.get("type") == "image_url":
                    image_url = item["image_url"]["url"]
                    if image_url.startswith("data:"):
                        # è½¬æ¢ base64 ä¸ºæœ¬åœ°æ–‡ä»¶è·¯å¾„
                        local_file_path = self._convert_base64_to_file(image_url)
                        processed_content.append({
                            "type": "image_url",
                            "image_url": {"url": local_file_path}
                        })
                        logger.info(f"ğŸ”„ base64 å›¾ç‰‡å·²è½¬æ¢ä¸ºæœ¬åœ°è·¯å¾„ï¼š{local_file_path}")
                    else:
                        # å·²ç»æ˜¯æ–‡ä»¶è·¯å¾„æˆ– URLï¼Œç›´æ¥ä½¿ç”¨
                        processed_content.append(item)
                else:
                    processed_content.append(item)
            return processed_content
        else:
            # çº¯æ–‡æœ¬å†…å®¹
            return content
    
    def _convert_messages_to_openai_format(self, messages: List[BaseMessage]) -> List[Dict]:
        """è½¬æ¢ LangChain æ¶ˆæ¯æ ¼å¼ä¸º OpenAI æ ¼å¼ï¼Œå¤„ç†å¤šæ¨¡æ€å†…å®¹"""
        formatted_messages = []
        for msg in messages:
            if isinstance(msg, SystemMessage):
                # ä¿æŒç³»ç»Ÿæ¶ˆæ¯åŸæ ·ï¼Œä¸æ·»åŠ å¢å¼ºå†…å®¹
                formatted_messages.append({"role": "system", "content": msg.content})
            elif isinstance(msg, HumanMessage):
                # å¤„ç†å¤šæ¨¡æ€å†…å®¹
                processed_content = self._process_multimodal_content(msg.content)
                formatted_messages.append({"role": "user", "content": processed_content})
            elif isinstance(msg, AIMessage):
                formatted_messages.append({"role": "assistant", "content": msg.content})
        
        return formatted_messages
    
    def _extract_json_from_markdown(self, content: str) -> str:
        """ä» markdown ä»£ç å—ä¸­æå– JSON å†…å®¹å¹¶ä¿®å¤æ ¼å¼é—®é¢˜"""
        import re
        import json
        
        original_content = content.strip()
        
        # å°è¯•åŒ¹é… ```json ... ``` æ ¼å¼
        json_match = re.search(r'```json\s*(.*?)\s*```', content, re.DOTALL)
        if json_match:
            extracted = json_match.group(1).strip()
            logger.info("ğŸ”§ ä» markdown ```json``` ä»£ç å—ä¸­æå–äº† JSON å†…å®¹")
            fixed_json = self._fix_malformed_json(extracted)
            return fixed_json
        
        # å°è¯•åŒ¹é… ``` ... ``` æ ¼å¼
        code_match = re.search(r'```\s*(.*?)\s*```', content, re.DOTALL)
        if code_match:
            extracted = code_match.group(1).strip()
            # ç®€å•æ£€æŸ¥æ˜¯å¦çœ‹èµ·æ¥åƒ JSON
            if extracted.strip().startswith('{') and extracted.strip().endswith('}'):
                logger.info("ğŸ”§ ä» markdown ``` ä»£ç å—ä¸­æå–äº† JSON å†…å®¹")
                fixed_json = self._fix_malformed_json(extracted)
                return fixed_json
        
        # å¦‚æœæ²¡æœ‰ä»£ç å—ï¼Œå°è¯•ä¿®å¤åŸå§‹å†…å®¹
        logger.info("ğŸ“ æ— éœ€ä» markdown æå–ï¼Œæ£€æŸ¥å¹¶ä¿®å¤åŸå§‹ JSON")
        fixed_json = self._fix_malformed_json(original_content)
        return fixed_json
    
    def _fix_malformed_json(self, json_str: str) -> str:
        """ä¿®å¤å¸¸è§çš„ JSON æ ¼å¼é—®é¢˜"""
        import json
        import re
        
        # é¦–å…ˆå°è¯•ç›´æ¥è§£æ
        try:
            json.loads(json_str)
            logger.info("âœ… JSON æ ¼å¼æ­£ç¡®ï¼Œæ— éœ€ä¿®å¤")
            return json_str
        except json.JSONDecodeError as e:
            logger.warning(f"âš ï¸ æ£€æµ‹åˆ° JSON æ ¼å¼é—®é¢˜: {e}")
            
            # ä¿®å¤å¸¸è§é—®é¢˜ï¼šå¤šä½™çš„é€—å·å’Œç»“æ„é”™è¯¯
            fixed = json_str
            
            # ä¿®å¤æ¨¡å¼: {"current_state": {...}}, "action": [...] -> {"current_state": {...}, "action": [...]}
            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ¥ç²¾ç¡®åŒ¹é…å’Œä¿®å¤ç»“æ„é—®é¢˜
            import re
            pattern = r'("current_state": \{.*?\})\}, "action":'
            replacement = r'\1, "action":'
            
            if re.search(pattern, fixed):
                logger.info("ğŸ”§ ä¿®å¤ current_state å’Œ action ä¹‹é—´çš„ç»“æ„é”™è¯¯")
                fixed = re.sub(pattern, replacement, fixed)
            
            # å°è¯•è§£æä¿®å¤åçš„ JSON
            try:
                json.loads(fixed)
                logger.info("âœ… JSON ä¿®å¤æˆåŠŸ")
                return fixed
            except json.JSONDecodeError:
                logger.warning("âŒ JSON ä¿®å¤å¤±è´¥ï¼Œè¿”å›åŸå§‹å†…å®¹")
                return json_str
    
    def _call_nexa_api(self, messages: List[BaseMessage], **kwargs) -> str:
        """è°ƒç”¨ Nexa API çš„æ ¸å¿ƒæ–¹æ³•"""
        # è½¬æ¢æ¶ˆæ¯æ ¼å¼
        formatted_messages = self._convert_messages_to_openai_format(messages)
        
        # å‡†å¤‡è¯·æ±‚è½½è·ï¼Œä½¿ç”¨é€‚ä¸­æ¸©åº¦ä»¥è·å¾—æ›´çµæ´»çš„è¾“å‡º
        payload = {
            "model": self.nexa_model,
            "messages": formatted_messages,
            "temperature": 0.5,  # æé«˜æ¸©åº¦ä»¥å¢å¼ºæŒ‡ä»¤éµå¾ªæ€§
            "max_tokens": kwargs.get("max_tokens", 2048)  # å¢åŠ æœ€å¤§ token æ•°
        }
        
        # è°ƒè¯•æ—¥å¿—
        logger.info(f"ğŸ” å¤„ç†å‰çš„æ¶ˆæ¯æ•°é‡: {len(messages)}")
        logger.info(f"ğŸ” å¤„ç†åçš„è½½è·: {json.dumps(payload, indent=2, ensure_ascii=False)[:1000]}...")
        
        try:
            # å°è¯•ä½¿ç”¨ chat/completions ç«¯ç‚¹
            response = requests.post(
                f"{self.nexa_base_url}/chat/completions",
                headers={"Content-Type": "application/json"},
                json=payload,
                timeout=60
            )
            response.raise_for_status()
            
            result = response.json()
            content = result["choices"][0]["message"]["content"]
            logger.info(f"âœ… Nexa API è°ƒç”¨æˆåŠŸï¼Œè¿”å›å†…å®¹é•¿åº¦: {len(content)}")
            logger.info(f"ğŸ“ æ¨¡å‹åŸå§‹è¾“å‡º: {content[:500]}...")
            
            # ä» markdown ä»£ç å—ä¸­æå– JSONï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
            processed_content = self._extract_json_from_markdown(content)
            return processed_content
            
        except Exception as e:
            logger.error(f"âŒ chat/completions è°ƒç”¨å¤±è´¥: {e}")
            # å›é€€åˆ° completions ç«¯ç‚¹
            try:
                # å°†æ¶ˆæ¯è½¬æ¢ä¸ºå•ä¸€æç¤º
                prompt = self._messages_to_prompt(formatted_messages)
                
                payload = {
                    "model": self.nexa_model,
                    "prompt": prompt,
                    "temperature": self.temperature,
                    "max_tokens": kwargs.get("max_tokens", 1024)
                }
                
                response = requests.post(
                    f"{self.nexa_base_url}/completions",
                    headers={"Content-Type": "application/json"},
                    json=payload,
                    timeout=60
                )
                response.raise_for_status()
                
                result = response.json()
                content = result["choices"][0]["text"].strip()
                logger.info(f"âœ… Nexa completions è°ƒç”¨æˆåŠŸï¼Œè¿”å›å†…å®¹é•¿åº¦: {len(content)}")
                logger.info(f"ğŸ“ æ¨¡å‹åŸå§‹è¾“å‡º (completions): {content[:500]}...")
                
                # ä» markdown ä»£ç å—ä¸­æå– JSONï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
                processed_content = self._extract_json_from_markdown(content)
                return processed_content
                
            except Exception as fallback_e:
                logger.error(f"âŒ completions å›é€€ä¹Ÿå¤±è´¥: {fallback_e}")
                raise ValueError(
                    f"Nexa API è°ƒç”¨å®Œå…¨å¤±è´¥ã€‚Chaté”™è¯¯: {e}, Completionsé”™è¯¯: {fallback_e}"
                )
    
    # é‡å†™æ‰€æœ‰å¯èƒ½çš„è°ƒç”¨æ–¹æ³•
    
    def invoke(
        self,
        input: LanguageModelInput,
        config: Optional[RunnableConfig] = None,
        *,
        stop: Optional[List[str]] = None,
        **kwargs: Any,
    ) -> AIMessage:
        """åŒæ­¥è°ƒç”¨æ–¹æ³•"""
        logger.info("ğŸ“ NexaChatLLM.invoke è¢«è°ƒç”¨")
        
        # è½¬æ¢è¾“å…¥ä¸ºæ¶ˆæ¯åˆ—è¡¨
        if isinstance(input, str):
            messages = [HumanMessage(content=input)]
        else:
            messages = input
        
        content = self._call_nexa_api(messages, **kwargs)
        return AIMessage(content=content)
    
    async def ainvoke(
        self,
        input: LanguageModelInput,
        config: Optional[RunnableConfig] = None,
        *,
        stop: Optional[List[str]] = None,
        **kwargs: Any,
    ) -> AIMessage:
        """å¼‚æ­¥è°ƒç”¨æ–¹æ³•"""
        logger.info(f"ğŸ“ NexaChatLLM.ainvoke è¢«è°ƒç”¨ï¼Œå‚æ•°: {list(kwargs.keys())}")
        
        # è¿‡æ»¤æ‰ä¸æ”¯æŒçš„å‚æ•°
        filtered_kwargs = {k: v for k, v in kwargs.items() if k not in ['response_format']}
        
        # åœ¨å¼‚æ­¥ç¯å¢ƒä¸­è°ƒç”¨åŒæ­¥æ–¹æ³•
        loop = asyncio.get_event_loop()
        
        # åˆ›å»ºä¸€ä¸ªåŒ…è£…å‡½æ•°æ¥æ­£ç¡®ä¼ é€’å‚æ•°
        def sync_call():
            return self.invoke(input, config, stop=stop, **filtered_kwargs)
        
        return await loop.run_in_executor(None, sync_call)
    
    def _generate(
        self,
        messages: List[BaseMessage],
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> ChatResult:
        """åŒæ­¥ç”Ÿæˆæ–¹æ³•"""
        logger.info("ğŸ“ NexaChatLLM._generate è¢«è°ƒç”¨")
        
        content = self._call_nexa_api(messages, **kwargs)
        return ChatResult(
            generations=[ChatGeneration(message=AIMessage(content=content))]
        )
    
    async def _agenerate(
        self,
        messages: List[BaseMessage],
        stop: Optional[List[str]] = None,
        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> ChatResult:
        """å¼‚æ­¥ç”Ÿæˆæ–¹æ³•"""
        logger.info(f"ğŸ“ NexaChatLLM._agenerate è¢«è°ƒç”¨ï¼Œå‚æ•°: {list(kwargs.keys())}")
        
        # è¿‡æ»¤æ‰ä¸æ”¯æŒçš„å‚æ•°
        filtered_kwargs = {k: v for k, v in kwargs.items() if k not in ['response_format']}
        
        # åœ¨å¼‚æ­¥ç¯å¢ƒä¸­è°ƒç”¨åŒæ­¥æ–¹æ³•
        loop = asyncio.get_event_loop()
        
        def sync_call():
            return self._call_nexa_api(messages, **filtered_kwargs)
        
        content = await loop.run_in_executor(None, sync_call)
        return ChatResult(
            generations=[ChatGeneration(message=AIMessage(content=content))]
        )
    
    def batch(
        self,
        inputs: List[LanguageModelInput],
        config: Optional[Union[RunnableConfig, List[RunnableConfig]]] = None,
        *,
        return_exceptions: bool = False,
        **kwargs: Any,
    ) -> List[AIMessage]:
        """æ‰¹é‡å¤„ç†æ–¹æ³•"""
        logger.info(f"ğŸ“ NexaChatLLM.batch è¢«è°ƒç”¨ï¼Œè¾“å…¥æ•°é‡: {len(inputs)}")
        
        results = []
        for input_item in inputs:
            try:
                result = self.invoke(input_item, **kwargs)
                results.append(result)
            except Exception as e:
                if return_exceptions:
                    results.append(e)
                else:
                    raise e
        
        return results
    
    async def abatch(
        self,
        inputs: List[LanguageModelInput],
        config: Optional[Union[RunnableConfig, List[RunnableConfig]]] = None,
        *,
        return_exceptions: bool = False,
        **kwargs: Any,
    ) -> List[AIMessage]:
        """å¼‚æ­¥æ‰¹é‡å¤„ç†æ–¹æ³•"""
        logger.info(f"ğŸ“ NexaChatLLM.abatch è¢«è°ƒç”¨ï¼Œè¾“å…¥æ•°é‡: {len(inputs)}")
        
        tasks = []
        for input_item in inputs:
            task = self.ainvoke(input_item, **kwargs)
            tasks.append(task)
        
        if return_exceptions:
            results = await asyncio.gather(*tasks, return_exceptions=True)
        else:
            results = await asyncio.gather(*tasks)
        
        return results
    
    def stream(
        self,
        input: LanguageModelInput,
        config: Optional[RunnableConfig] = None,
        *,
        stop: Optional[List[str]] = None,
        **kwargs: Any,
    ) -> Iterator[AIMessage]:
        """æµå¼å¤„ç†æ–¹æ³•ï¼ˆNexa å½“å‰ä¸æ”¯æŒæµå¼ï¼Œè¿”å›å®Œæ•´å“åº”ï¼‰"""
        logger.info("ğŸ“ NexaChatLLM.stream è¢«è°ƒç”¨ï¼ˆè½¬æ¢ä¸ºéæµå¼ï¼‰")
        
        result = self.invoke(input, config, stop=stop, **kwargs)
        yield result
    
    async def astream(
        self,
        input: LanguageModelInput,
        config: Optional[RunnableConfig] = None,
        *,
        stop: Optional[List[str]] = None,
        **kwargs: Any,
    ) -> AsyncIterator[AIMessage]:
        """å¼‚æ­¥æµå¼å¤„ç†æ–¹æ³•ï¼ˆNexa å½“å‰ä¸æ”¯æŒæµå¼ï¼Œè¿”å›å®Œæ•´å“åº”ï¼‰"""
        logger.info("ğŸ“ NexaChatLLM.astream è¢«è°ƒç”¨ï¼ˆè½¬æ¢ä¸ºéæµå¼ï¼‰")
        
        result = await self.ainvoke(input, config, stop=stop, **kwargs)
        yield result
    
    
    def _messages_to_prompt(self, messages: List[Dict[str, Any]]) -> str:
        """å°†æ¶ˆæ¯åˆ—è¡¨è½¬æ¢ä¸ºå•ä¸€æç¤ºå­—ç¬¦ä¸²ï¼ˆç”¨äº completions ç«¯ç‚¹ï¼‰"""
        prompt = ""
        for msg in messages:
            if msg["role"] == "system":
                prompt += f"System: {msg['content']}\n"
            elif msg["role"] == "user":
                if isinstance(msg["content"], list):
                    # å¤šæ¨¡æ€å†…å®¹ï¼Œåªæå–æ–‡æœ¬éƒ¨åˆ†
                    text_parts = [item["text"] for item in msg["content"] if item.get("type") == "text"]
                    content = " ".join(text_parts) if text_parts else "å›¾ç‰‡æè¿°è¯·æ±‚"
                    prompt += f"User: {content}\n"
                else:
                    prompt += f"User: {msg['content']}\n"
            elif msg["role"] == "assistant":
                prompt += f"Assistant: {msg['content']}\n"
        prompt += "Assistant: "
        return prompt


def create_nexa_llm(model: str, base_url: str, temperature: float = 0.0, **kwargs) -> NexaChatLLM:
    """
    åˆ›å»º Nexa LLM å®ä¾‹çš„å·¥å‚å‡½æ•°
    """
    logger.info(f"ğŸ­ åˆ›å»º Nexa LLM å®ä¾‹: model={model}, base_url={base_url}, temperature={temperature}")
    return NexaChatLLM(model=model, base_url=base_url, temperature=temperature, **kwargs)